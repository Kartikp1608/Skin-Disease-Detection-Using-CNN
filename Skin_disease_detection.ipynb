{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677b6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, save_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e23531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc9a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dceaa30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(6, activation='softmax')(x)  # Adjust the number of units based on the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc2ad918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44485f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4568d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-3:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819ccb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3de65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),  # Define the input shape with the Input layer\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(9, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0298103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd64b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 222, 222, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 111, 111, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 111, 111, 32)      0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 109, 109, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 54, 54, 64)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 52, 52, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               11075712  \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 9)                 297       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11181385 (42.65 MB)\n",
      "Trainable params: 11180489 (42.65 MB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23a52ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2239 images belonging to 9 classes.\n",
      "Found 118 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation and Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, \n",
    "                                   horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(r\"C:\\Users\\karti\\Desktop\\Skin cancer ISIC The International Skin Imaging Collaboration\\Train\", target_size=(224, 224), \n",
    "                                                    batch_size=64, class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(r\"C:\\Users\\karti\\Desktop\\Skin cancer ISIC The International Skin Imaging Collaboration\\Test\", target_size=(224, 224), \n",
    "                                                        batch_size=64, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65785435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices: {'actinic keratosis': 0, 'basal cell carcinoma': 1, 'dermatofibroma': 2, 'melanoma': 3, 'nevus': 4, 'pigmented benign keratosis': 5, 'seborrheic keratosis': 6, 'squamous cell carcinoma': 7, 'vascular lesion': 8}\n",
      "Class actinic keratosis has 114 images\n",
      "Class basal cell carcinoma has 376 images\n",
      "Class dermatofibroma has 95 images\n",
      "Class melanoma has 438 images\n",
      "Class nevus has 357 images\n",
      "Class pigmented benign keratosis has 462 images\n",
      "Class seborrheic keratosis has 77 images\n",
      "Class squamous cell carcinoma has 181 images\n",
      "Class vascular lesion has 139 images\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\Users\\karti\\Desktop\\Skin cancer ISIC The International Skin Imaging Collaboration\\Train\"\n",
    "\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "for cls in train_generator.class_indices:\n",
    "    cls_dir = os.path.join(data_dir, cls)\n",
    "    num_images = len(os.listdir(cls_dir))\n",
    "    print(f\"Class {cls} has {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38253eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 2.182261208576998, 1: 0.6616430260047281, 2: 2.6187134502923977, 3: 0.5679857940131913, 4: 0.6968565203859322, 5: 0.5384800384800384, 6: 3.230880230880231, 7: 1.3744628606507059, 8: 1.7897681854516387}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1baaa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True,verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1f4184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 3.5254 - accuracy: 0.1242 - val_loss: 4.6832 - val_accuracy: 0.1356 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 3.3406 - accuracy: 0.1577 - val_loss: 3.3613 - val_accuracy: 0.1356 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 3.2429 - accuracy: 0.1724 - val_loss: 3.3839 - val_accuracy: 0.1356 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 132s 4s/step - loss: 3.1038 - accuracy: 0.1818 - val_loss: 3.1611 - val_accuracy: 0.1017 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 133s 4s/step - loss: 2.9300 - accuracy: 0.2095 - val_loss: 3.0602 - val_accuracy: 0.1271 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 132s 4s/step - loss: 2.8112 - accuracy: 0.2318 - val_loss: 2.9572 - val_accuracy: 0.1949 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 2.7160 - accuracy: 0.2590 - val_loss: 2.9172 - val_accuracy: 0.1441 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 135s 4s/step - loss: 2.6094 - accuracy: 0.2608 - val_loss: 3.0351 - val_accuracy: 0.1441 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 135s 4s/step - loss: 2.5409 - accuracy: 0.2635 - val_loss: 3.4746 - val_accuracy: 0.1356 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 118s 3s/step - loss: 2.4398 - accuracy: 0.2930 - val_loss: 3.2985 - val_accuracy: 0.1441 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 79s 2s/step - loss: 2.3940 - accuracy: 0.2988 - val_loss: 2.9443 - val_accuracy: 0.1610 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 88s 3s/step - loss: 2.3018 - accuracy: 0.3001 - val_loss: 2.7930 - val_accuracy: 0.1780 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 88s 3s/step - loss: 2.2845 - accuracy: 0.3050 - val_loss: 2.7368 - val_accuracy: 0.2034 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 95s 3s/step - loss: 2.1779 - accuracy: 0.3131 - val_loss: 2.8615 - val_accuracy: 0.1780 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 92s 3s/step - loss: 2.1603 - accuracy: 0.3260 - val_loss: 2.8499 - val_accuracy: 0.2288 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 2.1143 - accuracy: 0.3256 - val_loss: 2.6533 - val_accuracy: 0.1864 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 2.0805 - accuracy: 0.3314 - val_loss: 2.2504 - val_accuracy: 0.2966 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 2.0584 - accuracy: 0.3506 - val_loss: 2.3480 - val_accuracy: 0.2712 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 2.0263 - accuracy: 0.3555 - val_loss: 2.4331 - val_accuracy: 0.3051 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 87s 2s/step - loss: 2.0232 - accuracy: 0.3327 - val_loss: 2.2712 - val_accuracy: 0.2203 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 89s 3s/step - loss: 1.9713 - accuracy: 0.3662 - val_loss: 2.4014 - val_accuracy: 0.1949 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - ETA: 0s - loss: 2.0200 - accuracy: 0.3457\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "35/35 [==============================] - 89s 3s/step - loss: 2.0200 - accuracy: 0.3457 - val_loss: 2.3208 - val_accuracy: 0.2712 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 91s 3s/step - loss: 1.9488 - accuracy: 0.3720 - val_loss: 2.1767 - val_accuracy: 0.3475 - lr: 2.0000e-04\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 85s 2s/step - loss: 1.9102 - accuracy: 0.3685 - val_loss: 2.1896 - val_accuracy: 0.3390 - lr: 2.0000e-04\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 84s 2s/step - loss: 1.9057 - accuracy: 0.3694 - val_loss: 2.1390 - val_accuracy: 0.3644 - lr: 2.0000e-04\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 87s 2s/step - loss: 1.8675 - accuracy: 0.3725 - val_loss: 2.1466 - val_accuracy: 0.3390 - lr: 2.0000e-04\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 86s 2s/step - loss: 1.8870 - accuracy: 0.3765 - val_loss: 2.1033 - val_accuracy: 0.3898 - lr: 2.0000e-04\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 86s 2s/step - loss: 1.8673 - accuracy: 0.3586 - val_loss: 2.0963 - val_accuracy: 0.3559 - lr: 2.0000e-04\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 87s 2s/step - loss: 1.8604 - accuracy: 0.3841 - val_loss: 2.0416 - val_accuracy: 0.3814 - lr: 2.0000e-04\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 94s 3s/step - loss: 1.8575 - accuracy: 0.3622 - val_loss: 2.0494 - val_accuracy: 0.3390 - lr: 2.0000e-04\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 89s 3s/step - loss: 1.8172 - accuracy: 0.3895 - val_loss: 2.0656 - val_accuracy: 0.3814 - lr: 2.0000e-04\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 91s 3s/step - loss: 1.8243 - accuracy: 0.3837 - val_loss: 2.0758 - val_accuracy: 0.3305 - lr: 2.0000e-04\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 86s 2s/step - loss: 1.8109 - accuracy: 0.3765 - val_loss: 2.0352 - val_accuracy: 0.3220 - lr: 2.0000e-04\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.8064 - accuracy: 0.3747 - val_loss: 2.0097 - val_accuracy: 0.3220 - lr: 2.0000e-04\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.8066 - accuracy: 0.3752 - val_loss: 2.0329 - val_accuracy: 0.3559 - lr: 2.0000e-04\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7844 - accuracy: 0.3747 - val_loss: 2.0693 - val_accuracy: 0.3390 - lr: 2.0000e-04\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7787 - accuracy: 0.3841 - val_loss: 2.0906 - val_accuracy: 0.3136 - lr: 2.0000e-04\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7868 - accuracy: 0.3845 - val_loss: 2.0743 - val_accuracy: 0.3729 - lr: 2.0000e-04\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7605 - accuracy: 0.3984 - val_loss: 1.9901 - val_accuracy: 0.3475 - lr: 2.0000e-04\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7995 - accuracy: 0.3930 - val_loss: 2.0688 - val_accuracy: 0.3644 - lr: 2.0000e-04\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7646 - accuracy: 0.4002 - val_loss: 2.0451 - val_accuracy: 0.2966 - lr: 2.0000e-04\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 84s 2s/step - loss: 1.7739 - accuracy: 0.3845 - val_loss: 1.9926 - val_accuracy: 0.3475 - lr: 2.0000e-04\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7664 - accuracy: 0.3939 - val_loss: 2.0268 - val_accuracy: 0.3475 - lr: 2.0000e-04\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - ETA: 0s - loss: 1.7383 - accuracy: 0.4038\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "35/35 [==============================] - 84s 2s/step - loss: 1.7383 - accuracy: 0.4038 - val_loss: 2.0130 - val_accuracy: 0.3559 - lr: 2.0000e-04\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7550 - accuracy: 0.3912 - val_loss: 2.0059 - val_accuracy: 0.3983 - lr: 4.0000e-05\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 83s 2s/step - loss: 1.7407 - accuracy: 0.4011 - val_loss: 2.0114 - val_accuracy: 0.3644 - lr: 4.0000e-05\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 84s 2s/step - loss: 1.7375 - accuracy: 0.3810 - val_loss: 2.0088 - val_accuracy: 0.3729 - lr: 4.0000e-05\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 86s 2s/step - loss: 1.7282 - accuracy: 0.3921 - val_loss: 1.9815 - val_accuracy: 0.3898 - lr: 4.0000e-05\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 94s 3s/step - loss: 1.7184 - accuracy: 0.4122 - val_loss: 1.9746 - val_accuracy: 0.3983 - lr: 4.0000e-05\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 92s 3s/step - loss: 1.7179 - accuracy: 0.3975 - val_loss: 1.9839 - val_accuracy: 0.4068 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "history = model.fit(train_generator, epochs=50, validation_data=validation_generator, callbacks=[reduce_lr,early_stopping],class_weight = class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a06cc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "model.save(\"trained_model.keras\")\n",
    "\n",
    "# Save the model in TensorFlow SavedModel format\n",
    "model.save('model.h5')\n",
    "\n",
    "# Load the model from TensorFlow SavedModel format\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac6025d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118 images belonging to 9 classes.\n",
      "2/2 [==============================] - 6s 3s/step - loss: 1.9839 - accuracy: 0.4068\n",
      "Test accuracy: 40.68%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on Test Data\n",
    "test_generator = test_datagen.flow_from_directory(r\"C:\\Users\\karti\\Desktop\\Skin cancer ISIC The International Skin Imaging Collaboration\\Test\", target_size=(224, 224), \n",
    "                                                  batch_size=64, class_mode='categorical', shuffle=True)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c61f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "# Get the ground truth labels\n",
    "test_labels = test_generator.classes\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "985bd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the classes\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "179f2a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "         actinic keratosis       0.08      0.12      0.10        16\n",
      "      basal cell carcinoma       0.12      0.06      0.08        16\n",
      "            dermatofibroma       0.07      0.06      0.07        16\n",
      "                  melanoma       0.00      0.00      0.00        16\n",
      "                     nevus       0.19      0.38      0.25        16\n",
      "pigmented benign keratosis       0.09      0.12      0.11        16\n",
      "      seborrheic keratosis       0.00      0.00      0.00         3\n",
      "   squamous cell carcinoma       0.33      0.12      0.18        16\n",
      "           vascular lesion       0.00      0.00      0.00         3\n",
      "\n",
      "                  accuracy                           0.12       118\n",
      "                 macro avg       0.10      0.10      0.09       118\n",
      "              weighted avg       0.12      0.12      0.11       118\n",
      "\n",
      "[[2 1 2 0 5 4 0 1 1]\n",
      " [3 1 1 0 3 4 2 0 2]\n",
      " [5 2 1 0 2 3 1 1 1]\n",
      " [2 0 4 0 7 0 2 0 1]\n",
      " [3 2 2 0 6 3 0 0 0]\n",
      " [3 1 0 1 7 2 0 2 0]\n",
      " [1 1 0 0 0 1 0 0 0]\n",
      " [4 0 3 0 1 5 1 2 0]\n",
      " [1 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the classes\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "# Print classification report\n",
    "print(classification_report(test_labels, predicted_classes, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(test_labels, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbe6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
